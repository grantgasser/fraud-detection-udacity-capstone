{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fraud Detection \n* Author: Grant Gasser\n* Last Edit: 8/20/2019\n* Kaggle: \"In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target `isFraud.`\""},{"metadata":{},"cell_type":"markdown","source":"## Summary (Disclaimer: this kernel has become a bit unorthodox - Enter at your own risk!)\n**This is my first serious attempt at a Kaggle competition. As such, I would really appreciate some feedback or tips for improving performance. If you enjoyed this notebook and it helped you, please leave a thumbs up! Though I've written most of the code myself, I have found the other public kernels very helpful and would encourage you do browse through them to look for other good ideas.**\n\n* **Public Leaderboard Results:** (I plan on using some of the previous submission files for ensembling)\n* Random Forest filled NaNs with -999: `.872`\n* XGBoost filled NaNs with -999: `.938`, submission file: `baseline_xgboost.csv`\n* XGBoost impute mean for numerical NaNs and most common cat for categorical NaNs, also normalized numerical vars: `.878`\n* XGBoost impute mean for numerical NaNs and most common cat for categorical NaNs, no normalization: `.932`, submission file: `preprocessed_xgboost`\n* XGBoost, impute mean for numerical NaNs, do not impute most common category for categorical NaNs, no normalization: `.934`, file: `preprocessed2_xgboost`. **NOTE**: imputing mean for numerical NaNs and most common category for categorical NaNs did not seem to help for XGBoost. \n* Version 21: hyperparameter tuning with XGBoost (Grid Search or Random Search), `.9284`, `xgboost_with_tuning`\n* `.9226`, `xgboost_with_tuning2`\n* Ensembling: `.9392`"},{"metadata":{},"cell_type":"markdown","source":"## ENSEMBLING:\n* Averaging out my previous predictions using the files listed above ^ \n* data: `https://www.kaggle.com/grantgasser/previous-submissions`"},{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For data analysis, model building, evaluating\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import preprocessing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 200) # before I forget\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#For loading data\nimport os\n\n# For plots\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## View provided files"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))\n\ninput_path = '../input'\n\n%matplotlib inline\n\nRANDOM_SEED = 42\nnan_replace = -999","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble (Random Forest & XGBoost)\n* Load previous submissions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(os.listdir('../input/previous-submissions'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline_random_forest = pd.read_csv('../input/previous-submissions/baseline_random_forest.csv', index_col='TransactionID') # .878\n# baseline_xgboost = pd.read_csv('../input/previous-submissions/baseline_xgboost.csv', index_col='TransactionID') # .938\n# xgboost_with_tuning = pd.read_csv('../input/previous-submissions/xgboost_with_tuning.csv', index_col='TransactionID') # .928\n# preprocessed2_xgboost = pd.read_csv('../input/previous-submissions/preprocessed2_xgboost.csv', index_col='TransactionID') # .934","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assert(baseline_random_forest.shape == baseline_xgboost.shape == xgboost_with_tuning.shape == preprocessed2_xgboost.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Weighted Avg\n* Based on scores (more weight for model outputs that had better scores)\n* `.05, .05, .1, .8` -> `.9392`, minor improvement from the best model's score of `.9381`, file: `ensemble5.csv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensemble = .05*baseline_random_forest + .05*xgboost_with_tuning + .01*preprocessed2_xgboost + .8*baseline_xgboost\n# ensemble.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensemble.to_csv('ensemble5.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Description \n* As provided by VESTA: https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-586800\n\n#### Transaction Table\n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n** Categorical Features: **\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* Pemaildomain Remaildomain\n* M1 - M9\n\n---\n\n#### Identity Table\nVariables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. \nThey're collected by Vesta’s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n** Categorical Features: **\n* DeviceType\n* DeviceInfo\n* id12 - id38"},{"metadata":{},"cell_type":"markdown","source":"## Load and explore data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_identity =  pd.read_csv(os.path.join(input_path, 'train_identity.csv'), index_col='TransactionID')\ntrain_transaction = pd.read_csv(os.path.join(input_path, 'train_transaction.csv'), index_col='TransactionID')\ntest_identity = pd.read_csv(os.path.join(input_path, 'test_identity.csv'), index_col='TransactionID')\ntest_transaction = pd.read_csv(os.path.join(input_path, 'test_transaction.csv'), index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### View tables"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_identity shape:', train_identity.shape)\nprint('train_transaction shape:', train_transaction.shape)\nprint('test_identity shape:', test_identity.shape)\nprint('test_transaction shape:', test_transaction.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge identity and transaction tables\n* Per Kaggle: \"The data is broken into two files `identity` and `transaction`, which are joined by `TransactionID`. Not all transactions have corresponding identity information.\n* Merge identity and transaction tables with `TransactionID` as the key\"\n* Since \"not all transactions have corresponding identity information,\" we will use a (left) outer join, using pandas merge function since a key might not appear in both tables"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train_transaction, train_identity, how='left', on='TransactionID')\ntest = pd.merge(test_transaction, test_identity, how='left', on='TransactionID')\n\n# see if transaction and identity variables one train table (should be same for test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clear up RAM\ndel train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train shape:', train.shape)\nprint('test shape:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train = train.shape[0]\nnum_test = test.shape[0]\nnum_features = test.shape[1]\n\nprint('Test data is {:.2%}'.format(num_test/(num_train+num_test)), 'of total train/test data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Model Random Forest (.878) and XGBoost (.938)\n### No pre-processing other than NaN replacement with -999 and label encode"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Replacing missing values\n* Use `train_filled` and `test_filled` for random forest and XGBoost (NaNs replaced with -999)\n* Use `train` and `test` for neural network, will imput values for NaNs and normalize"},{"metadata":{"trusted":true},"cell_type":"code","source":"# store target\ny_train = train['isFraud']\ntrain = train.drop('isFraud', axis=1)\n\n# replace NaNs\ntrain_filled = train.fillna(nan_replace)\ntest_filled = test.fillna(nan_replace)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reduce memory usage before fit\n* Thanks to https://www.kaggle.com/iasnobmatsu/xgb-model-with-feature-engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# not going to use until neural net, so compress to use less RAM\nprint(train.memory_usage().sum() / 1024**3, 'GB')\nprint(test.memory_usage().sum() / 1024**3, 'GB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntrain_filled = reduce_mem_usage(train_filled)\ntest = reduce_mem_usage(test)\ntest_filled = reduce_mem_usage(test_filled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Encoding\n* Change categorical variable data to numbers so that computer can understand\n* e.g. if the encoding is: `['mastercard', 'discover', 'visa']` based on index, then data like `['visa', 'visa', 'mastercard', 'discover', 'mastercard']` would be encoded as `[2, 2, 0, 1, 0]`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in train.columns:\n    if train[f].dtype=='object' or test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -999 and no strings (label encoding)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Random Forest and XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf = xgb.XGBClassifier(n_estimators=500,\n    max_depth=9,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=nan_replace,\n    random_state=RANDOM_SEED,\n    tree_method='gpu_hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, max_depth=9, random_state=RANDOM_SEED, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nxgb_clf.fit(train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrf_clf.fit(train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate feature importance from Random Forest\n* Unfortunately, the easily interpretable features did not seem to be important to the random forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"for num, feature in sorted(zip(rf_clf.feature_importances_, train.columns), reverse=True):\n    print('{:.4f} - {}'.format(num,feature))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred = xgb_clf.predict_proba(test)[:,1]\nrf_pred = rf_clf.predict_proba(test)[:,1]\ndel xgb_clf, rf_clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('XGBoost predictions:\\n', xgb_pred[:5], '\\n\\nRandom Forest predictions:\\n', rf_pred[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA and Pre-processing\n### Next model will be Neural Network (3rd model in the ensemble)"},{"metadata":{},"cell_type":"markdown","source":"### Data Types\n* Before diving into EDA, look at data types of current features and see if they need to be changed"},{"metadata":{},"cell_type":"markdown","source":"**Categorical Features:**\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* Pemaildomain Remaildomain\n* M1 - M9\n* DeviceType\n* DeviceInfo\n* id12 - id38"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in train.columns[:20]:\n    print(feature, '\\t', train[feature].dtype)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thoughts\n* Some of these should not be numerical data (e.g. card1-card6 should be 'object' types, not int64 or float64)\n* The next few cells changes this"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = ['ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'DeviceInfo', 'isFraud']\n\n# add card1-card6\nfor i in range(1, 7):\n    cat_features.append('card'+str(i))\n    \n    \n# add M1-M9\nfor i in range(1, 10):\n    cat_features.append('M'+str(i))\n    \n    \n# add id12-38\nfor i in range(12, 39):\n    cat_features.append('id_'+str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical features to data type 'object'\ndef convert_to_object(df, cat_features):\n    \"\"\"\n    Converts features to data type 'object', so that all categorical features in dataframe are of type 'object'\n    \n    Args:\n        df (pd.Dataframe)\n        cat_features (list): the categorical features as strings\n        \n    Returns:\n        df (pd.Dataframe): where new df has categorical features as type 'object'\n    \"\"\"\n    for feature in cat_features:\n        if feature not in df.columns:\n            print('ERROR:', feature)\n        else:\n            df[feature] = df[feature].astype('object')\n                        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = convert_to_object(train, cat_features)\ntest = convert_to_object(test, cat_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## Analyze Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in train.columns[:20]:\n    if train[feature].dtype == 'object':\n        print(feature, '\\t Unique categories:', train[feature].describe()[1])\n        print('-'*40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ^Confirming our intuition that 1-Hot encoding would be too high dim"},{"metadata":{},"cell_type":"markdown","source":"## Replace NaNs with median or most common category\n* Mean for numerical features, most common category for categorical features. \n* **NOTE:** with fillna, replace and other pandas functions, make sure you set the variable, because it returns the transformed object\n * e.g. `df[feature] = df[feature].replace()` instead of just `df[feature].replace()`"},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_nans(df, val_to_be_replaced):\n    \"\"\"\n    Replaces missing values (NaNs or -999) with the mean (if numerical) and with most\n    common category (if categorical)\n    \n    Args:\n        df (pd.DataFrame)\n        \n    Returns:\n        df (pd.DataFrame): transformed dataframe\n    \"\"\"\n    # NOTE: fillna did not work well here, recommend using replace\n    print(val_to_be_replaced, type(val_to_be_replaced))\n    \n    for feature in df.columns:\n        # replace categorical variable with most frequent\n        if df[feature].dtype == 'object':\n            df[feature] = df[feature].replace(val_to_be_replaced, df[feature].value_counts().index[0]) # most common category\n        \n        # replace NaN in numerical columns with median\n        else:\n            df[feature] = df[feature].replace(val_to_be_replaced, df[feature].median()) # median\n            \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = replace_nans(train, np.nan) # if they were still NaNs and not -999, could pass np.nan as second argument\ntest = replace_nans(test, np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore Labels\n* Note the class imbalance\n* About 3.5% of train examples are fraudulent"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_fraud = y_train.sum()\n\nprint('# of fraudulent transactions:', num_fraud, '\\n# of training examples:', num_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar([1, 2], height=[num_fraud, num_train-num_fraud])\nplt.title('Class Imbalance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare fraud and non-fraud (within training set)\n1. Compare the difference in means of numerical features between the fraud and non-fraud transactions. \n\n2. Compare the difference in distributions of categorical features between the fraud and non-fraud transactions. \n "},{"metadata":{},"cell_type":"markdown","source":"### Look at a few fraudulent transactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fraud = train[y_train == 1]\ntrain_not_fraud = train[y_train == 0]\n\ntrain_fraud.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_mean_of_feature(df, feature):\n#     \"\"\"\n#     Calculates and returns mean value of a numerical feature variable\n    \n#     Args:\n#         df (pd.DataFrame): the dataframe\n#         feature (str): the name of the numerical feature/variable as a string\n        \n#     Returns:\n#         mean (float)\n#     \"\"\"\n#     return df[feature].mean()\n\n# def get_categorical_distribution_of_feature(df, feature):\n#     \"\"\"\n#     Calculates and returns distribution of a categorical feature variable\n    \n#     Args:\n#         df (pd.DataFrame): the dataframe\n#         feature (str): the name of the categorical feature/variable as a string\n        \n#     Returns:\n#         categorical dist (pd.Series)\n#     \"\"\"\n#     return df[feature].value_counts() / df[feature].value_counts().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def compare_dataframes(df1, df1_description, df2, df2_description):\n#     \"\"\"\n#     Analyze each feature and compare the difference between fraud and not fraud table\n    \n#     Args:\n#         train_fraud (pd.DataFrame): contains the fraudulent transactions\n#         train_not_fraud (pd.DataFrame): contains the non-fraud transactions\n        \n#     Returns:\n        \n#     \"\"\"\n    \n#     # features that look interesting from visual inspection\n#     features = ['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card4', 'card6', \n#                 'P_emaildomain', 'R_emaildomain', 'id_29', 'id_30', 'id_31', 'DeviceType', 'DeviceInfo']\n    \n#     # Use this if analyzing ALL features of dataframes\n#     # make sure have same features in both dataframes\n#     #assert(sorted(train_not_fraud.columns) == sorted(train_fraud.columns))\n#     #features = train_fraud.columns \n    \n#     for feature in features:\n#         # numerical feature\n#         if df1[feature].dtype == 'int64' or df1[feature].dtype == 'float64':\n#             print('\\nNumerical feature (' + str(df1_description), ')\\tFeature name:', feature, '\\nmean:', get_mean_of_feature(df1, feature))\n#             print('\\nNumerical feature (' + str(df2_description), ')\\tFeature name:', feature, '\\nmean:', get_mean_of_feature(df2, feature))\n#         # categorical feature\n#         elif df1[feature].dtype == 'object': # object, a string\n#             print('\\nCategorical feature(' + str(df1_description), ')\\tFeature name:', feature, '\\nDistribution:\\n', get_categorical_distribution_of_feature(df1,feature)[:10])\n#             print('\\nCategorical feature(' + str(df2_description), ')\\tFeature name:', feature, '\\nDistribution:\\n', get_categorical_distribution_of_feature(df2,feature)[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare_dataframes(train_fraud, 'Train Fraud', train_not_fraud, 'Train Not Fraud')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Clear up RAM (10.3GB -> 8.6GB)\n# del train_fraud, train_not_fraud","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare_dataframes(train, 'Train set', test, 'Test set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Note TransactionDT has no overlap\n* As mentioned: https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda\n* Not sure what to do here. Maybe transform so that each value is relative to its range?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.hist(train['TransactionDT'], label='train')\n# plt.hist(test['TransactionDT'], label='test')\n# plt.legend()\n# plt.title('Distribution of TransactionDT dates')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # could correct for time difference in later iteration, for now, just drop column\n# train.drop(['TransactionDT'], axis=1)\n# test.drop(['TransactionDT'], axis=1)\n\n# print('dropped TransactionDT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Takeaways from EDA\n### There are lots of missing values\n### There is significant class imbalance (Only ~20,000 out of 590,000 are fraudulent, or 3.5 %)\n* Thus, a classifier that always predicts not fraud (0) would have 96.5% accuracy (on the training set, the test set is similar)\n\n\n### TRAIN SET: Comparing means of numerical features among fraud and non-fraud transactions:\n* `TransactionDT` - fraudulent transactions 4.5% higher\n* `TransactionAmt` - fraudulent transactions 11% more expensive\n\n### TRAIN SET: Comparing distributions of categorical variables among fraud and non-fraud transactions:\n* Take a look at the above cell to see the comparison\n* Some of these may spurious, but with 20,000 fraudulent examples, they could imply something\n* `ProductCD` - 39% of fraud transactions are 'C', but only 11% of non-fraud transactions are 'C'\n* `card1` - looks similar\n* `card4` - distribution looks similar\n* `card6` - fraud transactions distributed evenly (52/48) between debit and credit whereas non-fraud transactions are mostly debit (76%)\n* `P_emaildomain` - 13% of fraud comes from hotmail email vs. 9% non-fraud is hotmail email \n* `R_emaildomain` - 60% of emails on receiving end of fraud are gmail vs. only 40% for non-fraud\n* `id_29` - 70% are 'Found' in the fraud examples vs. 52% in the non-fraud\n* `id_30` - Though MAC OS versions show up on non-fraud top 10, do not show up in top 10 for fraud, implying fraud less common on MAC\n* `DeviceType` - fraud was about evenly distributed (50/50) between mobile and desktop, most non-fraud on desktop (61%)\n* `DeviceInfo` - similar to what id_30 implied, MAC used for 11% of non-fraud transactions but just 3% of fraud transactions\n\n\n### Comparing train distribution and test distribution\n* Remember, train size is $560,000$ and test size is $500,000$\n* Other than `TransactionDT`, the distributions look similar\n* Note that since the test set is later in time, there are some features where the distributions are almost certain to be different\n* e.g. `id_31` represents the browser used. For the train set, the most common browser was **chrome 63** at 16%. In the test set, the most common was **chrome 70**.\n7 versions later and **chrome 63** did not even show up in the top 10 most common browser for the test set, unsurprisingly.\n* Should I drop `id_31` and other columns affected by time or let the model weight it?\n* Also, looking at `DeviceType`, 60% of transactions in the train set were done on desktop vs. 54% on desktop in test set. \nCould this represent the increasing usage of mobile? Is there that much of a time difference between the train and test set?"},{"metadata":{},"cell_type":"markdown","source":"# More pre-processing"},{"metadata":{},"cell_type":"markdown","source":"### Dropping features with >80% missing values"},{"metadata":{},"cell_type":"markdown","source":"### Leaving out for now"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop_cols = [c for c in train.columns if (train[c].isnull().sum() /  num_train) > .80]\n\n# # also dropping V107 (though float values and VESTA did not say it was categorical, it really looks categorical in {0,1})\n# # it caused problems in making predictions, after further analysis, it seemed to have weak correlation with target variable\n# drop_cols.append('V107')\n\n# print('Dropping', len(drop_cols), 'columns.')\n# print('Including...', drop_cols[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = train.drop(drop_cols, axis=1)\n# test = test.drop(drop_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize Variables\n* For speed of convergence and numerical stability\n* Also to ensure variables with larger numbers do not dominate the model (e.g. TransactionAmt)\n* Normalize numerical variables: $x_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}$ where $i$ is the row, $j$ is the column, $\\mu_j$ is the mean of the column and $\\sigma_j$ is the std of the col\n* After the transformation, we will have $\\mu_j = 0$ and $\\sigma_j = 1$ for each numerical column/feature $j$\n* Could also try Min-Max scaling too which gives $x_j \\in (0,1)$ for all $i$."},{"metadata":{"trusted":true},"cell_type":"code","source":"# def normalize(df):\n#     \"\"\"\n#     Normalize numerical variables\n    \n#     Args:\n#         df (pd.DataFrame): dataframe to be normalized\n        \n#     Returns:\n#         df (pd.Dataframe): dataframe where each column has mean 0\n#     \"\"\"\n#     for feature in df.columns:\n#         if df[feature].dtype != 'object': # if it is numerical\n#             mu = df[feature].mean()\n#             sd = df[feature].std()\n#             df[feature] = (df[feature] - mu) / sd\n            \n#             # verify mean is 0\n#             mu_after = df[feature].mean()\n#             #print(feature, mu_after) # checks out\n            \n#     return df\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Skip normalize to see effect on performance\n* Pre-processed XGBoost score `.878` vs. `.938` with no pre-processing\n* **Note:** After removing normalization for XGBoost, performance jumped from `.878` to `.932`. Normalization may only be necessary or helpful with neural nets and similar algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = normalize(train)\n# test = normalize(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Old XGBoost Hyperparameter tuning - skip down to Neural Net"},{"metadata":{},"cell_type":"markdown","source":"## XGBoost (Old Code)\n* https://developer.ibm.com/code/2018/06/20/handle-imbalanced-data-sets-xgboost-scikit-learn-python-ibm-watson-studio/\n* XGBoost is an extreme gradient boosting algorithm based on trees that tends to perform very well out of the box compared to other ML algorithms.\n* XGBoost is popular with data scientists and is one of the most common ML algorithms used in Kaggle Competitions.\n* XGBoost allows you to tune various parameters.\n* XGBoost allows parallel processing."},{"metadata":{},"cell_type":"markdown","source":"## Fit the XGBoost Classifier Again using Cross Validation (Old Code)\n* See how the performance differs after imputing values and normalizing data\n* The baseline score was `.938`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import xgboost as xgb\n# from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter tuning with GridSearch and RandomizedSearch\n* [XGBoost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n* [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) - **Takes too much RAM**, exhaustive search of the parameters, expensive but finds the optimal set\n* set `scale_pos_weight` to adjust for class imbalance, common to do `sum(neg samples) / sum(pos samples)` which would be about 30 in this data set\n* control overfitting: `max_depth`, `min_child_weight`, `gamma` per xgboost docs"},{"metadata":{},"cell_type":"markdown","source":"## TODO\n* GridSearchCV and RandomizedSearchCV take too much RAM\n* Will write my own grid search loop and be more efficient with RAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# y = y_train.astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # grid of parameters, use GridSearch to find best combination\n# n_estimators = [400, 550, 700]\n# gamma = [.5, 1, 3]\n# max_depth = [6, 8, 10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import time\n\n# start = time.time()\n\n# # try all combinations of parameters\n# for n_est in n_estimators:\n#     for md in max_depth: \n#         for g in gamma:               \n#                 # train/test split, hopefully with a large dataset this is sufficient to estimate roc auc\n#                 X_train, X_test, y_train, y_valid = train_test_split(train, y, test_size=.3, random_state=RANDOM_SEED, shuffle=True)\n                \n#                 # fit\n#                 clf = xgb.XGBClassifier(n_estimators=n_est,\n#                                         gamma=g,\n#                                         max_depth=md,\n#                                         missing=nan_replace,\n#                                         subsample=.8,\n#                                         colsample_bytree=.8,\n#                                         scale_pos_weight=20, # to correct for class imbalance\n#                                         random_state=RANDOM_SEED,\n#                                         tree_method='gpu_hist')\n                \n#                 # fit with these parameters\n#                 clf.fit(X_train, y_train)\n#                 del X_train, y_train\n                \n#                 # predict on test/ estimate roc_auc, pick model with\n#                 y_pred = clf.predict_proba(X_test)\n#                 del X_test, clf\n                \n#                 print(roc_auc_score(y_valid, y_pred[:,1]), 'with parameters n_estimators={}, max_depth={}, gamma={},'.format(n_est, md, g))\n#                 del y_valid, y_pred\n                \n#                 now = time.time()\n#                 print('ELAPSED TIME:', now-start, 'seconds')\n                \n#                 # print(train.memory_usage().sum() / 1024**3, 'GB')\n#                 # print(y.memory_usage() / 1024**3, 'GB\\n')\n                \n#                 # train = reduce_mem_usage(train)\n                \n#                 # give RAM time to clear\n#                 time.sleep(10)\n                ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ^ Stopped early\n* Changing `gamma` does not seem to affect the performance\n* Adding more estimators and more max depth will improve performance on a subset of the test set, but has not led to improvement on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# # define xgboost classifier\n# clf = xgb.XGBClassifier(n_estimators=400,\n#                             gamma=1,\n#                             max_depth=6,\n#                             missing=nan_replace,\n#                             subsample=.8,\n#                             colsample_bytree=.8,\n#                             scale_pos_weight=20, # to correct for class imbalance\n#                             random_state=RANDOM_SEED,\n#                             tree_method='gpu_hist')\n    \n# # fit classifier\n# clf.fit(train, y_train)\n# del train, y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'), index_col='TransactionID')\n\n# y_pred = clf.predict_proba(test)\n# del clf, test\n\n# sample_submission['isFraud'] = y_pred[:,1]\n# del y_pred\n# sample_submission.to_csv('xgboost_with_tuning2.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}